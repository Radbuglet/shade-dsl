use std::{
    ops::Range,
    sync::{Mutex, mpsc},
};

use ctx2d_utils::hash::FxHashMap;
use smallvec::SmallVec;
use thunderdome::{Arena, Index};

use super::{
    gfx_bundle::GfxContext,
    stream::{PositionedVecWriter, SliceStream, StreamWrite, StreamWriter},
};

// === DynamicBufferManager === //

#[derive(Debug, Clone)]
pub struct DynamicBufferOpts<'a> {
    pub label: Option<&'a str>,
    pub maintain_cpu_copy: bool,
    pub usages: wgpu::BufferUsages,
}

#[derive(Debug, Copy, Clone, Hash, Eq, PartialEq, Ord, PartialOrd)]
pub struct DynamicBufferHandle(pub Index);

#[derive(Debug)]
pub struct DynamicBufferManager {
    gfx: GfxContext,
    belt: ChunkStagingBelt,
    buffers: Arena<DynamicBuffer>,
}

#[derive(Debug)]
struct DynamicBuffer {
    label: Option<String>,
    usages: wgpu::BufferUsages,

    /// The active length of the buffer w.r.t un-flushed modifications.
    target_len: wgpu::BufferAddress,

    /// The used length of the `buffer` that was previously uploaded or `0` if no buffer has yet
    /// been mapped. If the buffer is truncated, this value will be reduced to ensure that truncated
    /// data from the `buffer` is not unnecessarily included in any resized buffers.
    buffer_len: wgpu::BufferAddress,

    /// The current instance buffer generated by the brush. This buffer persists between frames and
    /// is only `None` if we haven't yet created a backing buffer.
    buffer: Option<wgpu::Buffer>,

    /// A chunk-based staging belt writer to the target buffer.
    writer: ChunkBufferWriter,

    /// A CPU-local copy of the data contained by the buffer.
    local_copy: Option<Vec<u8>>,
}

impl DynamicBufferManager {
    pub fn new(gfx: GfxContext) -> Self {
        Self {
            gfx,
            belt: ChunkStagingBelt::new(1 << 16),
            buffers: Arena::new(),
        }
    }

    pub fn create(&mut self, opts: DynamicBufferOpts<'_>) -> DynamicBufferHandle {
        let DynamicBufferOpts {
            label,
            maintain_cpu_copy,
            usages,
        } = opts;

        assert!(usages.contains(wgpu::BufferUsages::COPY_DST));

        DynamicBufferHandle(self.buffers.insert(DynamicBuffer {
            label: label.map(|v| v.to_string()),
            usages,
            target_len: 0,
            buffer_len: 0,
            buffer: None,
            writer: ChunkBufferWriter::new(),
            local_copy: maintain_cpu_copy.then_some(Vec::new()),
        }))
    }

    pub fn destroy(&mut self, buffer: DynamicBufferHandle) {
        let Some(mut state) = self.buffers.remove(buffer.0) else {
            return;
        };

        state.writer.release(&mut self.belt);
    }

    pub fn len(&self, buffer: DynamicBufferHandle) -> wgpu::BufferAddress {
        self.buffers[buffer.0].target_len
    }

    pub fn write(
        &mut self,
        buffer: DynamicBufferHandle,
        offset: wgpu::BufferAddress,
        data: &impl StreamWrite,
    ) -> u64 {
        let state = &mut self.buffers[buffer.0];

        // Update the CPU copy if necessary
        if let Some(local_copy) = &mut state.local_copy {
            data.write_to(&mut PositionedVecWriter {
                target: local_copy,
                start: offset as usize,
            });
        }

        // Use a staging buffer to write into a buffer directly.
        let written = state
            .writer
            .write(&self.gfx.device, &mut self.belt, state.target_len, data);

        state.target_len = state.target_len.max(offset + written);

        written
    }

    pub fn extend(&mut self, buffer: DynamicBufferHandle, data: &impl StreamWrite) -> u64 {
        self.write(buffer, self.len(buffer), data)
    }

    pub fn truncate_to(&mut self, buffer: DynamicBufferHandle, to: wgpu::BufferAddress) {
        let state = &mut self.buffers[buffer.0];

        // Adjust virtual length
        assert!(to <= state.target_len);
        state.target_len = to;

        // Adjust buffer length to ensure that copies on buffer resize don't include extraneous
        // data.
        state.buffer_len = state.buffer_len.min(to);
    }

    pub fn truncate_by(&mut self, buffer: DynamicBufferHandle, by: wgpu::BufferAddress) {
        self.truncate_to(buffer, self.len(buffer) - by);
    }

    pub fn copy_using_local(
        &mut self,
        src_buffer: DynamicBufferHandle,
        src_offset: wgpu::BufferAddress,
        dst_buffer: DynamicBufferHandle,
        dst_offset: wgpu::BufferAddress,
        size: wgpu::BufferAddress,
    ) {
        const NO_COPY_ERR: &str = "source buffer does not maintain a CPU-local copy";

        let src_offset = usize::try_from(src_offset).unwrap();
        let dst_offset = usize::try_from(dst_offset).unwrap();
        let size = usize::try_from(size).unwrap();

        if src_buffer == dst_buffer {
            let state = &mut self.buffers[src_buffer.0];

            let local_copy = state.local_copy.as_mut().expect(NO_COPY_ERR);

            local_copy.copy_within(src_offset..(src_offset + size), dst_offset);

            // Use a staging buffer to write into a buffer directly.
            state.writer.write(
                &self.gfx.device,
                &mut self.belt,
                state.target_len,
                &SliceStream(&local_copy[src_offset..][..size]),
            );

            state.target_len = state
                .target_len
                .max((dst_offset + size) as wgpu::BufferAddress);
        } else {
            let (Some(src_state), Some(dst_state)) =
                self.buffers.get2_mut(src_buffer.0, dst_buffer.0)
            else {
                panic!();
            };

            let src_local_copy = src_state.local_copy.as_mut().expect(NO_COPY_ERR);

            // Update the local copy if applicable.
            if let Some(dst_local_copy) = &mut dst_state.local_copy {
                dst_local_copy[dst_offset..][..size]
                    .copy_from_slice(&src_local_copy[src_offset..][..size]);
            }

            // Use a staging buffer to write into a buffer directly.
            dst_state.writer.write(
                &self.gfx.device,
                &mut self.belt,
                dst_state.target_len,
                &SliceStream(&src_local_copy[src_offset..][..size]),
            );

            dst_state.target_len = dst_state
                .target_len
                .max((dst_offset + size) as wgpu::BufferAddress);
        }
    }

    pub fn swap_remove_using_local(
        &mut self,
        buffer: DynamicBufferHandle,
        offset: wgpu::BufferAddress,
        size: wgpu::BufferAddress,
    ) {
        self.copy_using_local(buffer, self.len(buffer) - size, buffer, offset, size);
        self.truncate_by(buffer, size);
    }

    pub fn clear(&mut self, buffer: DynamicBufferHandle) {
        self.truncate_to(buffer, 0);
    }

    pub fn local_mirror(&self, buffer: DynamicBufferHandle) -> &[u8] {
        self.buffers[buffer.0].local_copy.as_deref().unwrap()
    }

    pub fn flush(&mut self, encoder: &mut wgpu::CommandEncoder) {
        for (_handle, state) in &mut self.buffers {
            // Resize the underlying buffer to accommodate our data.
            if state
                .buffer
                .as_ref()
                .is_none_or(|buffer| buffer.size() < state.target_len)
            {
                let new_size = state.target_len; // TODO: Better sizing strategy
                let new_buffer = self.gfx.device.create_buffer(&wgpu::BufferDescriptor {
                    label: state.label.as_deref(),
                    size: new_size,
                    usage: state.usages,
                    mapped_at_creation: false,
                });

                if let Some(old_buffer) = state.buffer.as_ref().filter(|_| state.buffer_len > 0) {
                    encoder.copy_buffer_to_buffer(old_buffer, 0, &new_buffer, 0, state.buffer_len);
                }

                state.buffer = Some(new_buffer);
            }

            let Some(buffer) = &state.buffer else {
                unreachable!();
            };

            // Flush writes
            state.writer.flush(
                &mut self.belt,
                encoder,
                buffer,
                state.target_len,
                state.local_copy.as_deref(),
            );

            state.buffer_len = state.target_len;
        }
    }

    pub fn reclaim(&mut self) {
        self.belt.reclaim_all();
    }

    pub fn buffer(&self, buffer: DynamicBufferHandle) -> &wgpu::Buffer {
        self.buffers[buffer.0].buffer.as_ref().unwrap()
    }
}

// === Utilities === //

#[derive(Debug)]
pub struct ChunkStagingBelt {
    chunk_size: wgpu::BufferAddress,
    free_chunks: Vec<wgpu::Buffer>,
    free_chunk_recv: Mutex<mpsc::Receiver<wgpu::Buffer>>,
    free_chunk_send: mpsc::Sender<wgpu::Buffer>,
    reclaim_queue: Vec<wgpu::Buffer>,
    total_buffers: u32,
}

impl ChunkStagingBelt {
    pub fn new(chunk_size: wgpu::BufferAddress) -> Self {
        let (free_chunk_send, free_chunk_recv) = mpsc::channel();
        Self {
            chunk_size,
            free_chunks: Vec::new(),
            free_chunk_recv: Mutex::new(free_chunk_recv),
            free_chunk_send,
            reclaim_queue: Vec::new(),
            total_buffers: 0,
        }
    }

    pub fn chunk_size(&self) -> wgpu::BufferAddress {
        self.chunk_size
    }

    pub fn acquire_mapped(&mut self, device: &wgpu::Device) -> wgpu::Buffer {
        while let Ok(chunk) = self.free_chunk_recv.get_mut().unwrap().try_recv() {
            self.free_chunks.push(chunk);
        }

        if let Some(chunk) = self.free_chunks.pop() {
            return chunk;
        }

        let buffer = device.create_buffer(&wgpu::BufferDescriptor {
            label: Some(&format!("DynamicBuffer staging {}", self.total_buffers)),
            size: self.chunk_size,
            usage: wgpu::BufferUsages::COPY_SRC | wgpu::BufferUsages::MAP_WRITE,
            mapped_at_creation: true,
        });

        self.total_buffers += 1;

        buffer
    }

    pub fn queue_reclaim(&mut self, buffer: wgpu::Buffer) {
        self.reclaim_queue.push(buffer);
    }

    pub fn reclaim_all(&mut self) {
        for buffer in self.reclaim_queue.drain(..) {
            let tx = self.free_chunk_send.clone();

            buffer
                .clone()
                .slice(..)
                .map_async(wgpu::MapMode::Write, move |res| {
                    if let Err(err) = res {
                        tracing::warn!("failed to remap {buffer:?}: {err}");
                        buffer.destroy();
                        return;
                    }

                    if let Err(err) = tx.send(buffer) {
                        err.0.destroy();
                    }
                });
        }
    }
}

#[derive(Debug, Default)]
pub struct ChunkBufferWriter {
    /// Maps chunk base address to a buffer containing the chunk's staged data.
    chunk_map: FxHashMap<wgpu::BufferAddress, Index>,

    chunks: Arena<CbwChunk>,
    last_chunk: Option<(wgpu::BufferAddress, Index)>,
}

#[derive(Debug)]
struct CbwChunk {
    base: wgpu::BufferAddress,
    buffer: wgpu::Buffer,
    intervals: SmallVec<[Range<u32>; 1]>,
}

impl ChunkBufferWriter {
    pub fn new() -> Self {
        Self::default()
    }

    pub fn write(
        &mut self,
        device: &wgpu::Device,
        belt: &mut ChunkStagingBelt,
        offset: wgpu::BufferAddress,
        data: &impl StreamWrite,
    ) -> wgpu::BufferAddress {
        struct WriteTarget<'a> {
            writer: &'a mut ChunkBufferWriter,
            device: &'a wgpu::Device,
            belt: &'a mut ChunkStagingBelt,
            offset: wgpu::BufferAddress,
            total_written: wgpu::BufferAddress,
        }

        impl StreamWriter for WriteTarget<'_> {
            fn write(&mut self, mut data: &[u8]) {
                self.total_written += data.len() as wgpu::BufferAddress;

                while !data.is_empty() {
                    // Determine the chunk for this current transaction.
                    let chunk_base = self.offset / self.belt.chunk_size() * self.belt.chunk_size();
                    let chunk_rel = self.offset - chunk_base;
                    let write_sz =
                        (self.belt.chunk_size() - chunk_rel).min(data.len() as wgpu::BufferAddress);

                    let chunk = 'find_chunk: {
                        if let Some((last_base, last_chunk)) = self.writer.last_chunk {
                            if chunk_base == last_base {
                                break 'find_chunk last_chunk;
                            }
                        }

                        *self.writer.chunk_map.entry(chunk_base).or_insert_with(|| {
                            self.writer.chunks.insert(CbwChunk {
                                base: chunk_base,
                                buffer: self.belt.acquire_mapped(self.device),
                                intervals: SmallVec::new(),
                            })
                        })
                    };

                    self.writer.last_chunk = Some((chunk_base, chunk));

                    let chunk = &mut self.writer.chunks[chunk];

                    // Perform copy
                    let mut view = chunk.buffer.slice(..).get_mapped_range_mut();

                    view[chunk_rel as usize..(chunk_rel + write_sz) as usize]
                        .copy_from_slice(&data[..write_sz as usize]);

                    // Advance cursor
                    data = &data[write_sz as usize..];
                    self.offset += write_sz as wgpu::BufferAddress;

                    // Delete all intervals that overlap with us, extending our interval as we go.
                    let mut to_insert = chunk_rel as u32..(chunk_rel + write_sz) as u32;
                    let mut i = 0;

                    while i < chunk.intervals.len() {
                        let other = &chunk.intervals[i];

                        if other.start > to_insert.end && to_insert.start > other.end {
                            i += 1;
                            continue;
                        }

                        to_insert.start = to_insert.start.min(other.start);
                        to_insert.end = to_insert.end.max(other.end);

                        chunk.intervals.swap_remove(i);
                        // (do not increment `i` because we have a new element in this position)
                    }

                    chunk.intervals.push(to_insert);
                }
            }
        }

        let mut target = WriteTarget {
            writer: self,
            device,
            belt,
            offset,
            total_written: 0,
        };
        data.write_to(&mut target);
        target.total_written
    }

    pub fn flush(
        &mut self,
        belt: &mut ChunkStagingBelt,
        encoder: &mut wgpu::CommandEncoder,
        target: &wgpu::Buffer,
        target_len: wgpu::BufferAddress,
        local_copy: Option<&[u8]>,
    ) {
        self.chunk_map.clear();
        self.last_chunk = None;

        for (_chunk_handle, mut chunk) in self.chunks.drain() {
            // Don't upload chunks into truncated portions of the buffer.
            if let Some(max_rel) = chunk.base.checked_sub(target_len) {
                if max_rel < belt.chunk_size() {
                    let max_rel = max_rel as u32;
                    chunk.intervals.retain_mut(|range| {
                        range.start = range.start.min(max_rel);
                        range.end = range.end.min(max_rel);

                        range.end > range.start
                    });
                } else {
                    chunk.intervals.clear();
                }
            }

            // TODO: Coalesce intervals with small gaps between them if we have a local copy
            //  available to us? In the same step, align buffer with `local_copy` if unaligned.
            chunk.intervals.sort_by(|a, b| a.start.cmp(&b.start));

            // Perform uploads
            if !chunk.intervals.is_empty() {
                chunk.buffer.unmap();
            }

            for interval in chunk.intervals {
                encoder.copy_buffer_to_buffer(
                    &chunk.buffer,
                    interval.start.into(),
                    target,
                    chunk.base + u64::from(interval.start),
                    u64::from(interval.end - interval.start),
                );
            }

            // Release the buffer.
            belt.queue_reclaim(chunk.buffer);
        }
    }

    pub fn release(&mut self, belt: &mut ChunkStagingBelt) {
        self.chunk_map.clear();
        self.last_chunk = None;

        for (_addr, chunk) in self.chunks.drain() {
            belt.queue_reclaim(chunk.buffer);
        }
    }
}
